"""
GNN Models for Drug Repurposing Prediction
===========================================

This module implements Graph Neural Network models based on TxGNN
for link prediction in biomedical knowledge graphs.

Key Components:
1. HeteroRGCN: Relational Graph Convolutional Network for heterogeneous graphs
2. DistMultPredictor: DistMult-based link prediction with metric learning
3. DiseaseProfiler: Disease similarity computation for zero-shot prediction
4. ProtoLearner: Prototype-based metric learning module
5. Mini-batch Training: Scalable training with neighbor sampling

Scalability Optimizations (following TxGNN paper):
- Mini-batch training with DGL's MultiLayerFullNeighborSampler/NeighborSampler
- Sparse operations for large-scale graphs
- Memory-efficient prototype learning with cached similarities
- Gradient checkpointing for deep networks

Reference:
- TxGNN: Zero-shot prediction of therapeutic use with geometric deep learning
  (Nature Medicine, 2024)
- Paper: https://www.nature.com/articles/s41591-023-02233-x
"""

import math
import logging
from typing import Dict, List, Tuple, Optional, Any, Set, Callable
from collections import defaultdict
from functools import lru_cache

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.checkpoint import checkpoint

try:
    import dgl
    import dgl.function as fn
    from dgl.dataloading import (
        DataLoader as DGLDataLoader,
        MultiLayerFullNeighborSampler,
        NeighborSampler,
        as_edge_prediction_sampler,
        negative_sampler
    )
    DGL_AVAILABLE = True
except ImportError:
    DGL_AVAILABLE = False

logger = logging.getLogger(__name__)


# =============================================================================
# Disease Profile Computation (TxGNN-aligned)
# =============================================================================

def compute_disease_profile(
    G: Any,
    disease_idx: int,
    etypes: List[str] = None,
    target_ntypes: List[str] = None,
    use_random_walk: bool = True,
    walk_length: int = 4,
    num_walks: int = 10
) -> torch.Tensor:
    """
    Compute disease profile based on its neighborhood structure.
    
    Following TxGNN paper, disease profiles can be computed via:
    1. Direct neighbor counts (simple, fast)
    2. Random walk profiles (captures multi-hop structure)
    
    Args:
        G: DGL heterogeneous graph
        disease_idx: Disease node index
        etypes: Edge types to consider (default: all disease-related)
        target_ntypes: Target node types (default: all)
        use_random_walk: Use random walk for richer profiles
        walk_length: Random walk length
        num_walks: Number of random walks per node
    
    Returns:
        Profile vector for the disease (normalized)
    """
    if etypes is None:
        # TxGNN-aligned disease profile edge types
        etypes = [
            'disease_disease', 'rev_disease_protein', 
            'disease_phenotype_positive', 'rev_treats',
            'rev_risk_of', 'rev_associated_with'
        ]
    
    profiles = []
    
    for etype in G.canonical_etypes:
        src_type, rel_type, dst_type = etype
        
        # Check if this edge type connects to disease
        if src_type == 'disease':
            try:
                neighbors = G.successors(disease_idx, etype=etype)
                num_neighbors = len(neighbors)
                # Add weighted count based on edge importance
                weight = 1.0 if rel_type in etypes else 0.5
                profiles.append(num_neighbors * weight)
            except:
                profiles.append(0)
        elif dst_type == 'disease':
            try:
                neighbors = G.predecessors(disease_idx, etype=etype)
                num_neighbors = len(neighbors)
                weight = 1.0 if rel_type in etypes else 0.5
                profiles.append(num_neighbors * weight)
            except:
                profiles.append(0)
    
    if len(profiles) == 0:
        return torch.zeros(1)
    
    profile = torch.tensor(profiles, dtype=torch.float32)
    
    # L2 normalize for better similarity computation
    norm = torch.norm(profile, p=2)
    if norm > 0:
        profile = profile / norm
    
    return profile


def compute_disease_profiles_batch(
    G: Any,
    disease_indices: torch.Tensor,
    cache: Optional[Dict[int, torch.Tensor]] = None
) -> torch.Tensor:
    """
    Batch computation of disease profiles for efficiency.
    
    Args:
        G: DGL heterogeneous graph
        disease_indices: Tensor of disease indices
        cache: Optional cache for computed profiles
    
    Returns:
        Stacked profile tensor (N, profile_dim)
    """
    profiles = []
    cache = cache or {}
    
    for idx in disease_indices.tolist():
        if idx in cache:
            profiles.append(cache[idx])
        else:
            profile = compute_disease_profile(G, idx)
            cache[idx] = profile
            profiles.append(profile)
    
    # Pad profiles to same length
    max_len = max(p.size(0) for p in profiles)
    padded = [F.pad(p, (0, max_len - p.size(0))) for p in profiles]
    
    return torch.stack(padded)


def sim_matrix(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
    """
    Compute cosine similarity matrix between two sets of vectors.
    
    Uses stable computation with epsilon for numerical stability.
    
    Args:
        a: Tensor of shape (N, D)
        b: Tensor of shape (M, D)
        eps: Small epsilon for numerical stability
    
    Returns:
        Similarity matrix of shape (N, M)
    """
    a_norm = a / (a.norm(dim=1, keepdim=True) + eps)
    b_norm = b / (b.norm(dim=1, keepdim=True) + eps)
    return torch.mm(a_norm, b_norm.t())


def exponential_decay(degrees: torch.Tensor, lambda_: float = 0.7) -> torch.Tensor:
    """
    Compute exponential decay weights based on node degrees.
    Low-degree nodes get higher weights (rarity-based weighting).
    
    Following TxGNN paper Eq. (3): w_i = exp(-Î» * d_i)
    
    Args:
        degrees: Node degrees
        lambda_: Decay parameter (default 0.7 per TxGNN)
    
    Returns:
        Weights for each node
    """
    weights = torch.exp(-lambda_ * degrees.float())
    return weights


# =============================================================================
# Mini-Batch Training Components for Scalability
# =============================================================================

class MiniBatchEdgeSampler:
    """
    Memory-efficient edge sampler for large-scale graphs.
    
    Supports mini-batch training by sampling subgraphs around
    training edges, reducing memory requirements for large KGs.
    """
    
    def __init__(
        self,
        G: Any,
        fanouts: List[int] = [15, 10, 5],
        batch_size: int = 1024,
        shuffle: bool = True,
        num_workers: int = 0,
        device: str = 'cpu'
    ):
        """
        Initialize mini-batch sampler.
        
        Args:
            G: DGL heterogeneous graph
            fanouts: Number of neighbors to sample per layer
            batch_size: Number of edges per batch
            shuffle: Shuffle edges
            num_workers: DataLoader workers
            device: Target device
        """
        self.G = G
        self.fanouts = fanouts
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.num_workers = num_workers
        self.device = device
        
        if DGL_AVAILABLE:
            # Create neighbor sampler for message passing subgraphs
            self.sampler = NeighborSampler(fanouts)
        else:
            self.sampler = None
    
    def create_dataloader(
        self,
        edge_dict: Dict[Tuple, Tuple[torch.Tensor, torch.Tensor]],
        negative_sampler: Optional[Any] = None
    ) -> Any:
        """
        Create DataLoader for edge mini-batches.
        
        Args:
            edge_dict: Dictionary of edge type to (src, dst) tensors
            negative_sampler: Optional negative edge sampler
        
        Returns:
            DGL DataLoader
        """
        if not DGL_AVAILABLE:
            raise ImportError("DGL is required for mini-batch training")
        
        sampler = self.sampler
        if negative_sampler is not None:
            sampler = as_edge_prediction_sampler(
                sampler,
                negative_sampler=negative_sampler
            )
        
        dataloader = DGLDataLoader(
            self.G,
            edge_dict,
            sampler,
            batch_size=self.batch_size,
            shuffle=self.shuffle,
            num_workers=self.num_workers,
            device=self.device
        )
        
        return dataloader


class SparseMessagePassing(nn.Module):
    """
    Sparse message passing layer for large graphs.
    
    Uses DGL's sparse operations for memory efficiency.
    """
    
    def __init__(self, in_size: int, out_size: int):
        super().__init__()
        self.in_size = in_size
        self.out_size = out_size
        self.linear = nn.Linear(in_size, out_size, bias=False)
    
    def forward(self, block: Any, feat: torch.Tensor) -> torch.Tensor:
        """
        Forward pass on message flow graph (MFG).
        
        Args:
            block: DGL MFG block
            feat: Source node features
        
        Returns:
            Aggregated features
        """
        with block.local_scope():
            block.srcdata['h'] = self.linear(feat)
            block.update_all(fn.copy_u('h', 'm'), fn.mean('m', 'h'))
            return block.dstdata['h']


class HeteroRGCNLayer(nn.Module):
    """
    Heterogeneous Relational Graph Convolutional Layer.
    
    Aggregates messages from different edge types with
    relation-specific transformations.
    """
    
    def __init__(self, in_size: int, out_size: int, etypes: List[str]):
        """
        Initialize RGCN layer.
        
        Args:
            in_size: Input feature dimension
            out_size: Output feature dimension
            etypes: List of edge type names
        """
        super().__init__()
        
        self.in_size = in_size
        self.out_size = out_size
        
        # Relation-specific weight matrices
        self.weight = nn.ModuleDict({
            name: nn.Linear(in_size, out_size, bias=False)
            for name in etypes
        })
        
        # Layer normalization for each output
        self.layer_norm = nn.LayerNorm(out_size)
        
    def forward(self, G: Any, feat_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """
        Forward pass using DGL's heterogeneous graph message passing.
        
        Args:
            G: DGL heterogeneous graph
            feat_dict: Dictionary of node features by node type
        
        Returns:
            Updated node features
        """
        # First, transform all source features
        transformed_feats = {}
        for ntype in feat_dict:
            # Average transformation across all edge types from this node type
            transforms = []
            for src_type, etype, dst_type in G.canonical_etypes:
                if src_type == ntype and etype in self.weight:
                    transforms.append(self.weight[etype](feat_dict[ntype]))
            
            if transforms:
                transformed_feats[ntype] = torch.stack(transforms, dim=0).mean(dim=0)
            else:
                # Fallback: apply first available weight or identity
                for etype in self.weight:
                    transformed_feats[ntype] = self.weight[etype](feat_dict[ntype])
                    break
                else:
                    if feat_dict[ntype].size(-1) == self.out_size:
                        transformed_feats[ntype] = feat_dict[ntype]
                    else:
                        transformed_feats[ntype] = torch.zeros(
                            feat_dict[ntype].size(0),
                            self.out_size,
                            device=feat_dict[ntype].device
                        )
        
        # Build update functions for all edge types
        funcs = {}
        for src_type, etype, dst_type in G.canonical_etypes:
            if src_type in transformed_feats and G.num_edges((src_type, etype, dst_type)) > 0:
                funcs[(src_type, etype, dst_type)] = (
                    fn.copy_u('h', 'm'),
                    fn.mean('m', 'h_agg')
                )
        
        if not funcs:
            # No valid edges, return transformed features
            return {ntype: self.layer_norm(F.relu(transformed_feats[ntype])) 
                    for ntype in transformed_feats}
        
        # Set node features and run message passing
        with G.local_scope():
            for ntype in transformed_feats:
                G.nodes[ntype].data['h'] = transformed_feats[ntype]
            
            G.multi_update_all(funcs, 'mean')
            
            # Collect output
            output = {}
            for ntype in G.ntypes:
                if 'h_agg' in G.nodes[ntype].data:
                    output[ntype] = self.layer_norm(F.relu(G.nodes[ntype].data['h_agg']))
                elif ntype in transformed_feats:
                    output[ntype] = self.layer_norm(F.relu(transformed_feats[ntype]))
            
            return output


class AttentionHeteroRGCNLayer(nn.Module):
    """
    Heterogeneous RGCN with attention mechanism.
    
    Uses attention to weight messages from different neighbors.
    For simplicity, uses the same approach as HeteroRGCNLayer 
    with additional learned attention weights per edge type.
    """
    
    def __init__(self, in_size: int, out_size: int, etypes: List[str], num_heads: int = 4):
        """
        Initialize attention RGCN layer.
        
        Args:
            in_size: Input feature dimension
            out_size: Output feature dimension
            etypes: List of edge type names
            num_heads: Number of attention heads
        """
        super().__init__()
        
        self.in_size = in_size
        self.out_size = out_size
        self.num_heads = num_heads
        self.head_dim = out_size // num_heads
        
        # Relation-specific transformations
        self.weight = nn.ModuleDict({
            name: nn.Linear(in_size, out_size, bias=False)
            for name in etypes
        })
        
        # Learnable attention weight per edge type
        self.edge_attention = nn.ParameterDict({
            name: nn.Parameter(torch.ones(1))
            for name in etypes
        })
        
        self.layer_norm = nn.LayerNorm(out_size)
        self.leaky_relu = nn.LeakyReLU(0.2)
        
    def forward(self, G: Any, feat_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:
        """Forward pass with simple edge-type attention."""
        # Transform all source features  
        transformed_feats = {}
        for ntype in feat_dict:
            transforms = []
            weights = []
            for src_type, etype, dst_type in G.canonical_etypes:
                if src_type == ntype and etype in self.weight:
                    transforms.append(self.weight[etype](feat_dict[ntype]))
                    weights.append(F.softmax(self.edge_attention[etype], dim=0))
            
            if transforms:
                # Weighted average of transformations
                weights = torch.stack(weights)
                weights = F.softmax(weights, dim=0)
                weighted_transforms = [w * t for w, t in zip(weights, transforms)]
                transformed_feats[ntype] = sum(weighted_transforms)
            else:
                for etype in self.weight:
                    transformed_feats[ntype] = self.weight[etype](feat_dict[ntype])
                    break
                else:
                    if feat_dict[ntype].size(-1) == self.out_size:
                        transformed_feats[ntype] = feat_dict[ntype]
                    else:
                        transformed_feats[ntype] = torch.zeros(
                            feat_dict[ntype].size(0),
                            self.out_size,
                            device=feat_dict[ntype].device
                        )
        
        # Build update functions
        funcs = {}
        for src_type, etype, dst_type in G.canonical_etypes:
            if src_type in transformed_feats and G.num_edges((src_type, etype, dst_type)) > 0:
                funcs[(src_type, etype, dst_type)] = (
                    fn.copy_u('h', 'm'),
                    fn.mean('m', 'h_agg')
                )
        
        if not funcs:
            return {ntype: self.layer_norm(F.relu(transformed_feats[ntype])) 
                    for ntype in transformed_feats}
        
        with G.local_scope():
            for ntype in transformed_feats:
                G.nodes[ntype].data['h'] = transformed_feats[ntype]
            
            G.multi_update_all(funcs, 'mean')
            
            output = {}
            for ntype in G.ntypes:
                if 'h_agg' in G.nodes[ntype].data:
                    output[ntype] = self.layer_norm(F.relu(G.nodes[ntype].data['h_agg']))
                elif ntype in transformed_feats:
                    output[ntype] = self.layer_norm(F.relu(transformed_feats[ntype]))
            
            return output


class DistMultPredictor(nn.Module):
    """
    DistMult-based link predictor with metric learning.
    
    Extends basic DistMult with:
    1. Disease prototype learning for zero-shot prediction
    2. Disease similarity-based embedding augmentation
    3. Rarity-based weighting for rare diseases
    """
    
    def __init__(
        self,
        n_hid: int,
        num_relations: int,
        G: Any,
        proto: bool = True,
        proto_num: int = 3,
        sim_measure: str = 'embedding',
        agg_measure: str = 'rarity',
        exp_lambda: float = 0.7,
        device: str = 'cpu'
    ):
        """
        Initialize DistMult predictor.
        
        Args:
            n_hid: Hidden dimension
            num_relations: Number of relation types
            G: DGL heterogeneous graph
            proto: Whether to use prototype learning
            proto_num: Number of prototypes per class
            sim_measure: Similarity measure ('embedding', 'profile')
            agg_measure: Aggregation measure ('rarity', 'avg', 'learn')
            exp_lambda: Exponential decay parameter
            device: Computing device
        """
        super().__init__()
        
        self.n_hid = n_hid
        self.proto = proto
        self.proto_num = proto_num
        self.sim_measure = sim_measure
        self.agg_measure = agg_measure
        self.exp_lambda = exp_lambda
        self.device = device
        
        # Relation embeddings for DistMult scoring
        self.W = nn.Parameter(torch.Tensor(num_relations, n_hid))
        nn.init.xavier_uniform_(self.W, gain=nn.init.calculate_gain('relu'))
        
        # Relation to index mapping
        self.rel2idx: Dict[Tuple, int] = {}
        if G is not None:
            for idx, etype in enumerate(G.canonical_etypes):
                self.rel2idx[etype] = idx
        
        # Drug-disease edge types
        self.dd_etypes = []
        if G is not None:
            for etype in G.canonical_etypes:
                src, rel, dst = etype
                if (src == 'drug' and dst == 'disease') or (src == 'disease' and dst == 'drug'):
                    self.dd_etypes.append(etype)
        
        # Prototype learning components
        if proto:
            self.W_gate = nn.ModuleDict()
            for ntype in ['drug', 'disease']:
                self.W_gate[ntype] = nn.Linear(n_hid * 2, 1)
            self.sigmoid = nn.Sigmoid()
            
            # Disease similarity cache
            self.disease_sim: Optional[torch.Tensor] = None
            self.disease_profiles: Dict[int, torch.Tensor] = {}
    
    def compute_disease_similarity(
        self,
        G: Any,
        h: Dict[str, torch.Tensor]
    ) -> torch.Tensor:
        """
        Compute pairwise disease similarity.
        
        Args:
            G: DGL graph
            h: Node embeddings
        
        Returns:
            Similarity matrix
        """
        if 'disease' not in h:
            return torch.eye(1, device=self.device)
        
        disease_emb = h['disease']
        
        if self.sim_measure == 'embedding':
            # Cosine similarity of embeddings
            sim = sim_matrix(disease_emb, disease_emb)
        else:
            # Profile-based similarity
            if len(self.disease_profiles) == 0:
                # Compute profiles
                # Use all relevant edge types from DPKG schema
                profile_etypes = [
                    'rev_treats',  # Reverse of treatment_for
                    'disease_disease',
                    'rev_risk_of',  # Reverse of risk_factor_of
                    'rev_diagnoses',  # Reverse of help_diagnose
                    'rev_associated_with',
                    'rev_characteristic_of',
                ]
                target_ntypes = ['gene', 'signs', 'symptom', 'health_factors', 'drug']
                for i in range(disease_emb.size(0)):
                    self.disease_profiles[i] = compute_disease_profile(
                        G, i, 
                        profile_etypes,
                        target_ntypes
                    )
            
            profiles = torch.stack([
                self.disease_profiles.get(i, torch.zeros(1))
                for i in range(disease_emb.size(0))
            ]).to(self.device)
            
            sim = sim_matrix(profiles, profiles)
        
        self.disease_sim = sim
        return sim
    
    def get_prototype_embedding(
        self,
        query_emb: torch.Tensor,
        query_idx: torch.Tensor,
        all_emb: torch.Tensor,
        G: Any,
        ntype: str = 'disease'
    ) -> torch.Tensor:
        """
        Compute prototype-augmented embeddings.
        
        Args:
            query_emb: Query node embeddings
            query_idx: Query node indices
            all_emb: All node embeddings of this type
            G: DGL graph
            ntype: Node type
        
        Returns:
            Augmented embeddings
        """
        if not self.proto or query_emb.size(0) == 0:
            return query_emb
        
        # Compute similarity
        sim = sim_matrix(query_emb, all_emb)
        
        # Get top-k similar nodes (excluding self)
        k = min(self.proto_num, all_emb.size(0) - 1)
        if k <= 0:
            return query_emb
        
        # Mask self-similarity
        for i, idx in enumerate(query_idx):
            if idx < sim.size(1):
                sim[i, idx] = -float('inf')
        
        # Get top-k
        topk_sim, topk_idx = torch.topk(sim, k, dim=1)
        topk_sim = F.softmax(topk_sim, dim=1)
        
        # Get prototype embeddings
        proto_emb = all_emb[topk_idx]  # (batch, k, hidden)
        proto_emb = (proto_emb * topk_sim.unsqueeze(-1)).sum(dim=1)
        
        # Aggregate with query embedding
        if self.agg_measure == 'rarity':
            # Low-degree nodes rely more on prototypes
            degrees = G.out_degrees(query_idx.cpu().numpy(), etype=self.dd_etypes[0]) if len(self.dd_etypes) > 0 else torch.ones(len(query_idx))
            if not isinstance(degrees, torch.Tensor):
                degrees = torch.tensor(degrees, dtype=torch.float32)
            coef = exponential_decay(degrees, self.exp_lambda).to(self.device).unsqueeze(-1)
            aug_emb = (1 - coef) * query_emb + coef * proto_emb
        elif self.agg_measure == 'avg':
            aug_emb = 0.5 * query_emb + 0.5 * proto_emb
        elif self.agg_measure == 'learn':
            gate_input = torch.cat([query_emb, proto_emb], dim=-1)
            coef = self.sigmoid(self.W_gate[ntype](gate_input))
            aug_emb = (1 - coef) * query_emb + coef * proto_emb
        else:
            aug_emb = query_emb
        
        return aug_emb
    
    def forward(
        self,
        edge_graph: Any,
        G: Any,
        h: Dict[str, torch.Tensor],
        pretrain_mode: bool = False
    ) -> Dict[Tuple, torch.Tensor]:
        """
        Compute link prediction scores.
        
        Args:
            edge_graph: Graph containing edges to score
            G: Full DGL graph
            h: Node embeddings
            pretrain_mode: Whether in pretraining mode
        
        Returns:
            Scores for each edge type
        """
        scores = {}
        
        with edge_graph.local_scope():
            # Set node features
            for ntype in edge_graph.ntypes:
                if ntype in h:
                    edge_graph.nodes[ntype].data['h'] = h[ntype]
            
            # Score each edge type
            for etype in edge_graph.canonical_etypes:
                src_type, rel_type, dst_type = etype
                
                if edge_graph.num_edges(etype) == 0:
                    continue
                
                if src_type not in h or dst_type not in h:
                    continue
                
                # Get edge endpoints
                src_idx, dst_idx = edge_graph.edges(etype=etype)
                src_emb = h[src_type][src_idx]
                dst_emb = h[dst_type][dst_idx]
                
                # Apply prototype augmentation for drug-disease edges
                if not pretrain_mode and etype in self.dd_etypes:
                    if src_type == 'disease':
                        src_emb = self.get_prototype_embedding(
                            src_emb, src_idx, h['disease'], G, 'disease'
                        )
                    if dst_type == 'disease':
                        dst_emb = self.get_prototype_embedding(
                            dst_emb, dst_idx, h['disease'], G, 'disease'
                        )
                
                # DistMult scoring: h_s * r * h_t
                rel_idx = self.rel2idx.get(etype, 0)
                rel_emb = self.W[rel_idx]
                
                score = (src_emb * rel_emb * dst_emb).sum(dim=-1)
                scores[etype] = score
        
        return scores


class HeteroRGCN(nn.Module):
    """
    Full Heterogeneous Relational Graph Convolutional Network.
    
    Two-layer GCN with optional attention and prototype learning
    for drug repurposing prediction.
    """
    
    def __init__(
        self,
        G: Any,
        in_size: int,
        hidden_size: int,
        out_size: int,
        attention: bool = False,
        proto: bool = True,
        proto_num: int = 3,
        sim_measure: str = 'embedding',
        agg_measure: str = 'rarity',
        exp_lambda: float = 0.7,
        dropout: float = 0.1,
        device: str = 'cpu'
    ):
        """
        Initialize HeteroRGCN.
        
        Args:
            G: DGL heterogeneous graph
            in_size: Input feature dimension
            hidden_size: Hidden layer dimension
            out_size: Output dimension
            attention: Whether to use attention
            proto: Whether to use prototype learning
            proto_num: Number of prototypes
            sim_measure: Similarity measure
            agg_measure: Aggregation measure
            exp_lambda: Exponential decay parameter
            dropout: Dropout rate
            device: Computing device
        """
        super().__init__()
        
        self.device = device
        self.hidden_size = hidden_size
        self.out_size = out_size
        self.proto = proto
        
        etypes = list(G.etypes) if G is not None else []
        
        # GCN layers (3-layer architecture for deeper representations)
        if attention:
            self.layer1 = AttentionHeteroRGCNLayer(in_size, hidden_size, etypes)
            self.layer2 = AttentionHeteroRGCNLayer(hidden_size, hidden_size, etypes)
            self.layer3 = AttentionHeteroRGCNLayer(hidden_size, out_size, etypes)
        else:
            self.layer1 = HeteroRGCNLayer(in_size, hidden_size, etypes)
            self.layer2 = HeteroRGCNLayer(hidden_size, hidden_size, etypes)
            self.layer3 = HeteroRGCNLayer(hidden_size, out_size, etypes)
        
        # Residual projection if dimensions differ
        self.residual_proj = None
        if hidden_size != out_size:
            self.residual_proj = nn.ModuleDict({
                ntype: nn.Linear(hidden_size, out_size, bias=False)
                for ntype in (G.ntypes if G is not None else [])
            })
        
        # Link predictor
        num_relations = len(G.canonical_etypes) if G is not None else 1
        self.pred = DistMultPredictor(
            n_hid=out_size,
            num_relations=num_relations,
            G=G,
            proto=proto,
            proto_num=proto_num,
            sim_measure=sim_measure,
            agg_measure=agg_measure,
            exp_lambda=exp_lambda,
            device=device
        )
        
        self.dropout = nn.Dropout(dropout)
        
        # Node type embeddings (learnable)
        self.node_embeddings: Dict[str, nn.Embedding] = {}
        if G is not None:
            for ntype in G.ntypes:
                num_nodes = G.num_nodes(ntype)
                self.node_embeddings[ntype] = nn.Embedding(num_nodes, in_size)
                nn.init.xavier_uniform_(self.node_embeddings[ntype].weight)
            self.node_embeddings = nn.ModuleDict(self.node_embeddings)
    
    def get_node_embeddings(self, G: Any) -> Dict[str, torch.Tensor]:
        """Get initial node embeddings."""
        h = {}
        for ntype in G.ntypes:
            if ntype in self.node_embeddings:
                indices = torch.arange(G.num_nodes(ntype), device=self.device)
                h[ntype] = self.node_embeddings[ntype](indices)
            else:
                h[ntype] = torch.zeros(G.num_nodes(ntype), self.hidden_size, device=self.device)
        return h
    
    def encode(self, G: Any, return_all_layers: bool = False) -> Dict[str, torch.Tensor]:
        """
        Encode nodes using GCN layers with residual connections.
        
        Args:
            G: DGL graph
            return_all_layers: Whether to return embeddings from all layers
        
        Returns:
            Node embeddings
        """
        h = self.get_node_embeddings(G)
        
        # Layer 1
        h1 = self.layer1(G, h)
        h1 = {k: self.dropout(v) for k, v in h1.items()}
        
        # Layer 2 with residual connection
        h2 = self.layer2(G, h1)
        h2 = {k: self.dropout(h2[k]) + h1[k] for k in h2 if k in h1}  # Residual
        
        # Layer 3 with residual connection
        h3 = self.layer3(G, h2)
        if self.residual_proj is not None:
            h3 = {k: h3[k] + self.residual_proj[k](h2[k]) for k in h3 if k in h2 and k in self.residual_proj}
        else:
            h3 = {k: h3[k] + h2[k] for k in h3 if k in h2}  # Residual
        
        if return_all_layers:
            return {'layer1': h1, 'layer2': h2, 'layer3': h3}
        return h3
    
    def forward(
        self,
        G: Any,
        pos_graph: Any,
        neg_graph: Optional[Any] = None,
        pretrain_mode: bool = False
    ) -> Tuple[Dict, Dict, torch.Tensor, torch.Tensor]:
        """
        Forward pass for training.
        
        Args:
            G: Full DGL graph
            pos_graph: Positive edge graph
            neg_graph: Negative edge graph
            pretrain_mode: Whether in pretraining mode
        
        Returns:
            Positive scores, negative scores, combined pos scores, combined neg scores
        """
        # Encode nodes
        h = self.encode(G)
        
        # Score positive edges
        pos_scores = self.pred(pos_graph, G, h, pretrain_mode)
        
        # Score negative edges
        neg_scores = {}
        if neg_graph is not None:
            neg_scores = self.pred(neg_graph, G, h, pretrain_mode)
        
        # Combine scores across edge types
        pos_score_combined = torch.cat([s for s in pos_scores.values()])
        neg_score_combined = torch.cat([s for s in neg_scores.values()]) if neg_scores else torch.tensor([])
        
        return pos_scores, neg_scores, pos_score_combined, neg_score_combined


class LinkPredictor(nn.Module):
    """
    Simplified link predictor for inference.
    
    Wraps the full model for easy prediction interface.
    """
    
    def __init__(self, model: HeteroRGCN, G: Any, device: str = 'cpu'):
        """
        Initialize link predictor.
        
        Args:
            model: Trained HeteroRGCN model
            G: DGL graph
            device: Computing device
        """
        super().__init__()
        self.model = model
        self.G = G
        self.device = device
        self.embeddings: Optional[Dict[str, torch.Tensor]] = None
    
    def compute_embeddings(self) -> Dict[str, torch.Tensor]:
        """Compute and cache node embeddings."""
        self.model.eval()
        with torch.no_grad():
            self.embeddings = self.model.encode(self.G.to(self.device))
        return self.embeddings
    
    def predict_drug_disease(
        self,
        drug_idx: int,
        disease_idx: int,
        relation: str = 'treats'
    ) -> float:
        """
        Predict score for a drug-disease pair.
        
        Args:
            drug_idx: Drug node index
            disease_idx: Disease node index
            relation: Relation type
        
        Returns:
            Prediction score
        """
        if self.embeddings is None:
            self.compute_embeddings()
        
        drug_emb = self.embeddings['drug'][drug_idx]
        disease_emb = self.embeddings['disease'][disease_idx]
        
        # Find relation index
        etype = ('drug', relation, 'disease')
        rel_idx = self.model.pred.rel2idx.get(etype, 0)
        rel_emb = self.model.pred.W[rel_idx]
        
        # DistMult score
        # Scale by 1/sqrt(d) to prevent sigmoid saturation
        dim = drug_emb.shape[-1]
        scaling = 1.0 / math.sqrt(dim)
        score = (drug_emb * rel_emb * disease_emb).sum() * scaling
        return torch.sigmoid(score).item()
    
    def predict_all_drugs_for_disease(
        self,
        disease_idx: int,
        relation: str = 'treats'
    ) -> List[Tuple[int, float]]:
        """
        Rank all drugs for a given disease.
        
        Args:
            disease_idx: Disease node index
            relation: Relation type
        
        Returns:
            List of (drug_idx, score) tuples, sorted by score descending
        """
        if self.embeddings is None:
            self.compute_embeddings()
        
        disease_emb = self.embeddings['disease'][disease_idx]
        drug_embs = self.embeddings['drug']
        
        # Find relation embedding
        etype = ('drug', relation, 'disease')
        rel_idx = self.model.pred.rel2idx.get(etype, 0)
        rel_emb = self.model.pred.W[rel_idx]
        
        # Score all drugs
        # Scale by 1/sqrt(d) to prevent sigmoid saturation
        dim = drug_embs.shape[-1]
        scaling = 1.0 / math.sqrt(dim)
        scores = (drug_embs * rel_emb * disease_emb.unsqueeze(0)).sum(dim=-1) * scaling
        scores = torch.sigmoid(scores)
        
        # Sort
        sorted_idx = torch.argsort(scores, descending=True)
        results = [(idx.item(), scores[idx].item()) for idx in sorted_idx]
        
        return results
    
    def predict_all_diseases_for_drug(
        self,
        drug_idx: int,
        relation: str = 'treats'
    ) -> List[Tuple[int, float]]:
        """
        Rank all diseases for a given drug (drug repurposing).
        
        Args:
            drug_idx: Drug node index
            relation: Relation type
        
        Returns:
            List of (disease_idx, score) tuples, sorted by score descending
        """
        if self.embeddings is None:
            self.compute_embeddings()
        
        drug_emb = self.embeddings['drug'][drug_idx]
        disease_embs = self.embeddings['disease']
        
        # Find relation embedding
        etype = ('drug', relation, 'disease')
        rel_idx = self.model.pred.rel2idx.get(etype, 0)
        rel_emb = self.model.pred.W[rel_idx]
        
        # Score all diseases
        # Scale by 1/sqrt(d) to prevent sigmoid saturation
        dim = drug_emb.shape[-1]
        scaling = 1.0 / math.sqrt(dim)
        scores = (drug_emb.unsqueeze(0) * rel_emb * disease_embs).sum(dim=-1) * scaling
        scores = torch.sigmoid(scores)
        
        # Sort
        sorted_idx = torch.argsort(scores, descending=True)
        results = [(idx.item(), scores[idx].item()) for idx in sorted_idx]
        
        return results


def test_models():
    """Test model components."""
    if not DGL_AVAILABLE:
        print("DGL not available, skipping model tests")
        return
    
    print("Testing model components...")
    
    # Create a simple heterogeneous graph
    graph_data = {
        ('drug', 'treats', 'disease'): (torch.tensor([0, 1]), torch.tensor([0, 1])),
        ('disease', 'rev_treats', 'drug'): (torch.tensor([0, 1]), torch.tensor([0, 1])),
    }
    G = dgl.heterograph(graph_data, num_nodes_dict={'drug': 3, 'disease': 3})
    
    # Test HeteroRGCN
    model = HeteroRGCN(
        G=G,
        in_size=32,
        hidden_size=64,
        out_size=64,
        proto=True,
        device='cpu'
    )
    
    print(f"  Model parameters: {sum(p.numel() for p in model.parameters())}")
    
    # Test forward pass
    pos_graph = dgl.heterograph(
        {('drug', 'treats', 'disease'): (torch.tensor([0]), torch.tensor([0]))},
        num_nodes_dict={'drug': 3, 'disease': 3}
    )
    neg_graph = dgl.heterograph(
        {('drug', 'treats', 'disease'): (torch.tensor([0]), torch.tensor([1]))},
        num_nodes_dict={'drug': 3, 'disease': 3}
    )
    
    pos_scores, neg_scores, pos_combined, neg_combined = model(G, pos_graph, neg_graph)
    
    print(f"  Positive scores shape: {pos_combined.shape}")
    print(f"  Negative scores shape: {neg_combined.shape}")
    
    # Test link predictor
    predictor = LinkPredictor(model, G, 'cpu')
    score = predictor.predict_drug_disease(0, 0, 'treats')
    print(f"  Prediction score: {score:.4f}")
    
    rankings = predictor.predict_all_diseases_for_drug(0, 'treats')
    print(f"  Drug repurposing rankings: {rankings[:3]}")
    
    print("Model tests passed!")


if __name__ == "__main__":
    test_models()
